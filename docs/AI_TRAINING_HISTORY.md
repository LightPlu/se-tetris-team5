# 테트리스 AI 유전 알고리즘 훈련 이력 및 방법론

## 목차
1. [개요](#개요)
2. [실패했던 훈련 방법들](#실패했던-훈련-방법들)
3. [현재 진행 중인 훈련 방법](#현재-진행-중인-훈련-방법)
4. [훈련 결과 비교](#훈련-결과-비교)
5. [주요 개선 사항](#주요-개선-사항)
6. [참고 문서](#참고-문서)

---

## 개요

테트리스 AI의 가중치를 최적화하기 위해 유전 알고리즘(Genetic Algorithm)을 사용하여 훈련을 진행했습니다. 초기에는 단일 플레이어 모드 기반 평가를 사용했으나, 실제 대전 모드와의 환경 차이로 인해 실패했습니다. 이후 AI vs AI 대전 모드 기반 평가로 전환하여 실제 사용 환경과 동일한 조건에서 훈련을 진행하고 있습니다.

### 훈련 목표
- **대전 모드에서 승리할 수 있는 AI 가중치 최적화**
- **El-Tetris 알고리즘 기반 특징 계산 + 대전 모드 공격 규칙**

---

## 실패했던 훈련 방법들

### 1. 단일 플레이어 모드 기반 평가 (초기 시도)

#### 방법
- **평가 환경**: AI가 혼자서 게임을 플레이
- **적합도 계산**: 생존 시간, 줄 삭제 수, 점수 기반
- **문제점**: 실제 대전 모드와 환경이 완전히 다름

#### 실패 원인
1. **공격 메커니즘 부재**: 대전 모드에서는 2줄 이상 삭제 시 상대방에게 공격 블록이 전송되지만, 단일 플레이어 모드에서는 이를 시뮬레이션할 수 없음
2. **전략 차이**: 단일 플레이어 모드에서는 생존 중심 전략이 최적이지만, 대전 모드에서는 공격 중심 전략이 필요
3. **가중치 불일치**: 단일 플레이어 모드에서 최적화된 가중치가 대전 모드에서는 성능이 떨어짐

#### 결과
- 훈련된 AI가 대전 모드에서 제대로 작동하지 않음
- 1줄도 제대로 삭제하지 못하는 경우 발생

---

### 2. 단순 특징 기반 평가 (El-Tetris 미적용)

#### 방법
- **평가 특징**: Aggregate Height, Bumpiness, Lines Cleared, Tetris
- **문제점**: El-Tetris의 정교한 특징 계산 방식을 사용하지 않음

#### 실패 원인
1. **Landing Height vs Aggregate Height**: El-Tetris는 블록별 착지 높이를 계산하지만, 단순 Aggregate Height는 전체 보드 높이 합계만 계산
2. **Eroded Piece Cells Metric 부재**: El-Tetris의 핵심 특징인 EPCM을 사용하지 않음
3. **Well Sums 부재**: Well Building 전략을 위한 Well Sums 특징 미사용

#### 결과
- El-Tetris 논문의 가중치와 비교 시 큰 차이 발생
- AI의 보드 관리 능력이 떨어짐

---

### 3. 빠른 평가 모드 미적용 (초기)

#### 방법
- **평가 방식**: 모든 개체에 대해 정확한 평가만 수행
- **문제점**: 훈련 시간이 너무 오래 걸림 (6시간 이상)

#### 실패 원인
1. **시간 비효율**: 각 개체당 5게임씩 정확한 평가를 수행하여 세대당 수십 분 소요
2. **확장성 부족**: 세대 수나 개체 수를 늘릴 수 없음

#### 결과
- 훈련 시간이 너무 길어 실용적이지 않음
- 실험 및 개선을 반복하기 어려움

---

### 4. 이전 세대 최고 개체를 상대 AI로 사용하지 않음 (초기)

#### 방법
- **상대 AI**: 항상 기본 가중치(El-Tetris) 사용
- **문제점**: 점진적 난이도 증가가 없어 학습 효율이 떨어짐

#### 실패 원인
1. **학습 곡선 부재**: 초기 세대와 후기 세대 모두 같은 난이도의 상대와 대전
2. **로컬 최적해 수렴**: 약한 상대와의 대전으로 인해 최적화가 제대로 이루어지지 않음

#### 결과
- 학습 효율이 낮음
- 최적 가중치를 찾지 못함

---

## 현재 진행 중인 훈련 방법

### 평가 환경: AI vs AI 대전 모드

#### 핵심 특징
- **실제 대전 모드와 동일한 환경**: 공격 블록 전송, 게임 오버 조건 등 모든 규칙 동일
- **AI vs AI 대전**: 평가 대상 AI와 상대 AI가 대전하여 승리/패배 기반 적합도 계산

#### 평가 로직
```java
// 공격 로직: 블록 고정 후 콜백으로 처리
engine1.setOnBlockFixedCallback(() -> handleAttack(engine1, engine2));
engine2.setOnBlockFixedCallback(() -> handleAttack(engine2, engine1));

// 승리/패배 기반 적합도
if (winner == 1) {
    score += 10000;  // 승리 보너스
    linesCleared += 20;
    isWin = true;
} else if (winner == 2) {
    score = Math.max(0, score - 5000);  // 패배 패널티
    isWin = false;
}
```

---

### El-Tetris 알고리즘 기반 특징 계산

#### 구현된 특징
1. **Landing Height**: 블록이 착지한 높이 (블록별 계산)
2. **Eroded Piece Cells Metric (EPCM)**: 삭제된 줄 수 × 유용한 블록 수
3. **Row Transitions**: 가로 방향 빈칸↔채움 전환 횟수
4. **Column Transitions**: 세로 방향 빈칸↔채움 전환 횟수
5. **Number of Holes**: 구멍 개수
6. **Well Sums**: Well의 깊이 합

#### 대전 모드 추가 특징
7. **Attack Bonus**: 2줄 이상 삭제 시 공격 보너스

#### 평가 함수
```java
Score = weightLandingHeight × landingHeight
      + weightEPCM × epcm
      + weightRowTransitions × rowTransitions
      + weightColumnTransitions × columnTransitions
      + weightHoles × holes
      + weightWellSums × wellSums
      + (linesCleared >= 2 ? linesCleared × weightAttackBonus : 0)
```

---

### 빠른 평가 모드 + 정확한 평가 모드

#### 빠른 평가 모드 (세대 내 모든 개체)
- **목적**: 빠른 학습을 위한 대략적 평가
- **설정**:
  - 게임당 최대 시간: 3초
  - 최대 이동 횟수: 1000회
  - 게임 타이머 간격: 20ms
  - AI 행동 주기: 5ms
  - 개체당 게임 수: 5게임

#### 정확한 평가 모드 (각 세대 최고 개체)
- **목적**: 정확한 성능 측정
- **설정**:
  - 게임당 최대 시간: 10초
  - 최대 이동 횟수: 2000회
  - 게임 타이머 간격: 50ms
  - AI 행동 주기: 20ms
  - 개체당 게임 수: 5게임

---

### 적응적 상대 AI (이전 세대 최고 개체)

#### 방법
- **첫 세대**: 기본 가중치(El-Tetris)와 대전
- **2세대 이후**: 이전 세대의 최고 개체와 대전
- **효과**: 점진적 난이도 증가로 학습 효율 향상

#### 구현
```java
if (currentGeneration > 0 && !generationResults.isEmpty()) {
    Individual previousBest = new Individual(
        copyWeights(generationResults.get(generationResults.size() - 1).bestWeights));
    evaluator.setOpponent(previousBest);
} else {
    evaluator.setOpponent(null);  // 기본 가중치 사용
}
```

---

### 유전 알고리즘 파라미터

#### 현재 설정
- **집단 크기 (POPULATION_SIZE)**: 30
- **엘리트 개체 수 (ELITE_SIZE)**: 6 (20%)
- **최대 세대 수 (MAX_GENERATIONS)**: 10
- **기본 돌연변이 확률 (BASE_MUTATION_RATE)**: 0.15
- **교차 확률 (CROSSOVER_RATE)**: 0.7

#### 적응적 돌연변이
- 세대가 진행될수록 돌연변이율 감소 (0.5% 감소)
- 초기: 탐색 중심, 후기: 수렴 중심

---

### 적합도 함수

#### 계산 방식
```java
fitness = survivalBonus          // totalGames * 5.0
       + linesBonus              // averageLinesPerGame * 100.0
       + scoreBonus              // averageScorePerGame * 0.5
       + winBonus                // winCount * 1000.0
       - lossPenalty             // lossCount * 500.0
       + earlyGameOverPenalty    // (score == 0 ? -100.0 : 0.0)
```

#### 특징
- **승리 보너스**: 승리 시 큰 보너스 (1000점)
- **패배 패널티**: 패배 시 패널티 (500점)
- **줄 삭제 보너스**: 평균 줄 삭제 수에 큰 가중치 (100점)
- **생존 보너스**: 게임 수에 작은 가중치 (5점)

---

## 훈련 결과 비교

### 보고서 1: 2025-11-30 12:55:00 (초기 시도)

#### 설정
- 세대 수: 50세대
- 집단 크기: 20
- 평가 방식: 단일 플레이어 모드 (추정)

#### 결과
- **최고 적합도**: 45.70
- **평균 줄 수**: 0.3
- **평균 점수**: 124
- **가중치**: 
  - Holes: -77.61
  - Row Transitions: -10.54
  - Aggregate Height: -0.33
  - Bumpiness: -0.37
  - Lines Cleared: 13.03
  - Tetris: 52.47

#### 문제점
- 평균 줄 수가 매우 낮음 (0.3줄)
- 적합도가 매우 낮음
- 대전 모드에서 제대로 작동하지 않음

---

### 보고서 2: 2025-11-30 13:45:41 (개선 시도)

#### 설정
- 세대 수: 50세대
- 집단 크기: 20
- 평가 방식: AI vs AI 대전 모드 (추정)

#### 결과
- **최고 적합도**: (데이터 부족)
- **평균 줄 수**: (데이터 부족)
- **평균 점수**: (데이터 부족)

#### 문제점
- 데이터가 불완전하여 정확한 평가 불가

---

### 보고서 3: 2025-11-30 14:19:02 (AI vs AI 대전 모드 적용)

#### 설정
- 세대 수: 50세대
- 집단 크기: 20
- 평가 방식: AI vs AI 대전 모드
- 총 소요 시간: 21.2분

#### 결과
- **최고 적합도**: 45.70
- **평균 줄 수**: 0.3
- **평균 점수**: 124
- **가중치**: 
  - Holes: -77.61
  - Row Transitions: -10.54
  - Aggregate Height: -0.33
  - Bumpiness: -0.37
  - Lines Cleared: 13.03
  - Tetris: 52.47

#### 문제점
- 여전히 평균 줄 수가 매우 낮음
- 적합도가 낮음
- El-Tetris 특징을 사용하지 않음

---

### 보고서 4: 29세대까지 진행 (El-Tetris 미적용, 이전 방식)

#### 설정
- 세대 수: 30세대 (0-29)
- 집단 크기: 30
- 평가 방식: AI vs AI 대전 모드
- 특징: Aggregate Height, Bumpiness 사용 (El-Tetris 미적용)

#### 결과
- **최고 적합도**: 2109.00 (세대 7)
- **평균 줄 수**: 21.0
- **평균 점수**: 10490
- **가중치**: 
  - Holes: -65.19
  - Row Transitions: -8.79
  - Aggregate Height: -0.81
  - Bumpiness: -0.99
  - Lines Cleared: 11.68
  - Tetris: 137.71

#### 평가
- 평균 줄 수가 크게 개선됨 (0.3 → 21.0)
- 적합도가 크게 향상됨 (45.70 → 2109.00)
- 하지만 El-Tetris 특징을 사용하지 않아 논문과 비교 시 차이 발생

---

### 보고서 5: 2025-12-01 13:33:48 (El-Tetris 알고리즘 기반 훈련 완료)

#### 설정
- **세대 수**: 10세대
- **집단 크기**: 30
- **엘리트 개체 수**: 6
- **평가 방식**: AI vs AI 대전 모드
- **특징**: El-Tetris 6가지 특징 + 대전 모드 공격 보너스
- **상대 AI**: 이전 세대 최고 개체
- **빠른 평가**: 세대 내 모든 개체
- **정확한 평가**: 각 세대 최고 개체
- **총 소요 시간**: 166.7분 (약 2.8시간)

#### 최적화된 타이머 설정
- **빠른 평가**:
  - 게임 타이머: 20ms
  - AI 행동 주기: 5ms
  - 최대 게임 시간: 3초
  - 최대 이동 횟수: 1000회
  
- **정확한 평가**:
  - 게임 타이머: 50ms
  - AI 행동 주기: 20ms
  - 최대 게임 시간: 10초
  - 최대 이동 횟수: 2000회

#### 결과
- **최고 적합도**: 3330.40 (정확한 평가 기준, 세대 6)
- **평균 줄 수**: 8.0
- **평균 점수**: 4011
- **가중치**:
  - Landing Height: -9.47
  - EPCM: 3.41
  - Row Transitions: -9.08
  - Column Transitions: -16.46
  - Holes: -6.73
  - Well Sums: -2.68
  - Attack Bonus: 15.42

#### 평가
- ✅ El-Tetris 알고리즘 특징 계산 구현 완료
- ✅ 대전 모드 공격 규칙 추가 완료
- ✅ 빠른 평가 모드 최적화 완료
- ⚠️ 평균 줄 수가 이전 방법(21.0줄)보다 낮음 (8.0줄)
- ⚠️ 적합도가 이전 방법(2109.00)보다 높지만, 평균 줄 수는 낮음
- **추가 최적화 필요**: 적합도 함수나 평가 환경 조정 필요

#### 주요 관찰
- 빠른 평가 적합도는 매우 높음 (12000+), 하지만 정확한 평가에서는 낮음 (3000+)
- 이는 빠른 평가와 정확한 평가 간의 환경 차이를 시사
- 세대 6에서 최고 성능 (적합도 12048.50, 평균 줄 20.0, 평균 점수 10047)

---

## 주요 개선 사항

### 1. 평가 환경 개선
- ❌ 단일 플레이어 모드 → ✅ AI vs AI 대전 모드
- 실제 사용 환경과 동일한 조건에서 평가

### 2. 특징 계산 개선
- ❌ 단순 특징 (Aggregate Height, Bumpiness) → ✅ El-Tetris 특징 (Landing Height, EPCM, Column Transitions, Well Sums)
- 논문 기반 정교한 특징 계산

### 3. 평가 방식 개선
- ❌ 정확한 평가만 수행 (6시간 소요) → ✅ 빠른 평가 + 정확한 평가 (50-100분)
- 학습 시간 대폭 단축

### 4. 상대 AI 개선
- ❌ 항상 기본 가중치 사용 → ✅ 이전 세대 최고 개체 사용
- 점진적 난이도 증가로 학습 효율 향상

### 5. 적합도 함수 개선
- 승리/패배 기반 보너스/패널티 추가
- 줄 삭제 보너스 가중치 증가
- 생존 보너스 가중치 감소

### 6. 대전 모드 공격 규칙 추가
- 2줄 이상 삭제 시 공격 보너스 추가
- 실제 대전 모드와 동일한 공격 메커니즘

---

## 참고 문서

### 관련 문서
1. **RUN_GENETIC_ALGORITHM.md**: 유전 알고리즘 실행 방법
2. **weight_comparison_analysis.md**: El-Tetris vs 유전 알고리즘 가중치 비교 분석

### 보고서 파일
모든 보고서는 `docs/reports/` 디렉토리에 저장되어 있습니다.

1. **report_20251130_125500_initial_attempt.txt**: 초기 시도 (50세대)
2. **report_20251130_134541_improved_attempt.txt**: 개선 시도 (50세대)
3. **report_20251130_141902_ai_vs_ai.txt**: AI vs AI 대전 모드 적용 (50세대)
4. **report_29generations_old_method.txt**: 29세대까지 진행 (El-Tetris 미적용)
5. **report_20251201_133348_el_tetris_based.txt**: El-Tetris 알고리즘 기반 훈련 (10세대) ⭐ 최신

자세한 내용은 [reports/README.md](./reports/README.md)를 참고하세요.

### 코드 파일
1. **GeneticAlgorithm.java**: 유전 알고리즘 메인 로직
2. **FitnessEvaluator.java**: 적합도 평가 (AI vs AI 대전 모드)
3. **TetrisAI.java**: AI 로직 (El-Tetris 특징 계산)
4. **WeightSet.java**: 가중치 집합
5. **Individual.java**: 개체 (가중치 + 적합도)
6. **GeneticAlgorithmRunner.java**: 실행 클래스

---

## 결론

초기에는 단일 플레이어 모드 기반 평가와 단순 특징을 사용하여 실패했습니다. 이후 AI vs AI 대전 모드 기반 평가와 El-Tetris 알고리즘 특징 계산을 적용하여 실제 사용 환경과 동일한 조건에서 훈련을 진행했습니다. 빠른 평가 모드와 적응적 상대 AI를 통해 학습 효율을 크게 향상시켰습니다.

El-Tetris 알고리즘 기반 훈련(2025-12-01 완료)에서는 6가지 특징과 대전 모드 공격 규칙을 모두 반영하여 훈련을 진행했습니다. 세대 6에서 최고 성능(평균 줄 20.0, 평균 점수 10047)을 기록했으나, 최종 평균 줄 수(8.0)는 이전 방법(21.0)보다 낮았습니다. 이는 빠른 평가와 정확한 평가 간의 환경 차이, 또는 적합도 함수의 조정이 필요함을 시사합니다.

### 향후 개선 방향
1. **적합도 함수 조정**: 평균 줄 수에 더 큰 가중치 부여
2. **평가 환경 통일**: 빠른 평가와 정확한 평가 간의 환경 차이 최소화
3. **더 많은 세대 훈련**: 10세대에서 30-50세대로 확장
4. **하이퍼파라미터 튜닝**: 집단 크기, 돌연변이율, 교차율 등 조정

---

## 최신 훈련 결과 요약

### El-Tetris 알고리즘 기반 훈련 (2025-12-01 완료)

**최고 성능 (세대 6)**:
- 적합도: 12048.50 (빠른 평가) / 12048.50 (정확한 평가)
- 평균 줄 수: 20.0
- 평균 점수: 10047

**최종 가중치**:
```java
new WeightSet(-9.47, 3.41, -9.08, -16.46, -6.73, -2.68, 15.42)
```

**El-Tetris 논문 가중치와 비교**:
- Landing Height: -9.47 vs -4.50 (약 2.1배)
- EPCM: 3.41 vs 3.42 (거의 동일)
- Row Transitions: -9.08 vs -3.22 (약 2.8배)
- Column Transitions: -16.46 vs -9.35 (약 1.8배)
- Holes: -6.73 vs -7.90 (약 0.85배)
- Well Sums: -2.68 vs -3.39 (약 0.79배)

**주요 차이점**:
- 대전 모드 특성상 공격 관련 가중치(Column Transitions, Row Transitions)가 더 큼
- Well Sums는 El-Tetris와 유사한 수준
- EPCM은 논문과 거의 동일

---

## 생존 능력 향상 훈련 (2025-12-01)

### 배경
기존 훈련된 AI가 실제 플레이어와 대전할 때 생존 능력이 부족하여 플레이어가 아무것도 하지 않아도 AI가 먼저 게임 오버되는 문제가 발생했습니다.

### 개선 사항

#### 1. 적합도 계산 방식 개선
- **생존 시간 보너스 추가**: 평균 생존 시간(초) × 10.0
- **생존 보너스 증가**: 게임 수 × 5.0 → 20.0
- **승리 보너스 감소**: 승리 횟수 × 1000.0 → 500.0 (생존과의 균형)
- **패배 패널티 증가**: 패배 횟수 × 500.0 → 1000.0
- **조기 게임 오버 패널티 증가**: -100.0 → -500.0
- **장기 생존 보너스 추가**: 평균 19초(실제 5분) 이상 생존 시 +1000.0

#### 2. 평가 시간 확장
- **정밀 평가 최대 시간**: 10초 → 20초 (실제 5분 = 평가 19초)
- **빠른 평가 최대 시간**: 3초 → 10초 (실제 5분 = 평가 8초)
- 생존 시간 보너스를 받을 수 있도록 충분한 시간 확보

#### 3. 가중치 범위 조정
- **Holes 가중치 범위 증가**: -21 ~ -1 → -20 ~ -5 (더 큰 패널티로 생존 능력 강화)
- **공격 보너스 범위 감소**: 
  - Attack 2 Lines: 5 ~ 35 → 5 ~ 25
  - Attack 3 Lines: 10 ~ 50 → 10 ~ 40
  - Attack 4 Lines: 20 ~ 80 → 20 ~ 60

#### 4. 유전 알고리즘 설정
- **집단 크기**: 30 → 20
- **세대 수**: 10 → 5
- 빠른 테스트를 위한 설정

### 적합도 계산 공식 (개선 후)

```
적합도 = 생존시간보너스 + 생존보너스 + 줄삭제보너스 + 점수보너스
       + 승리보너스 - 패배패널티 + 조기게임오버패널티 + 장기생존보너스

생존시간보너스 = 평균생존시간(초) × 10.0
생존보너스 = 게임수 × 20.0
줄삭제보너스 = 평균줄수 × 100.0
점수보너스 = 평균점수 × 0.5
승리보너스 = 승리횟수 × 500.0
패배패널티 = 패배횟수 × 1000.0
조기게임오버패널티 = (점수 == 0) ? -500.0 : 0.0
장기생존보너스 = (평균생존시간 > 19초) ? 1000.0 : 0.0
```

### 목표
- AI가 최소한 5분(평가 환경 19초) 이상 생존할 수 있도록 학습
- 공격과 생존의 균형 유지
- 플레이어가 아무것도 하지 않아도 AI가 먼저 게임 오버되지 않도록 개선

---

---

## 하이브리드 학습 방법 (2025-12-01)

### 배경
기존 통합 적합도 계산 방식의 문제점:
- 평균 시간 보너스: 공격이 강해 일찍 끝나면 시간이 짧아 불리함
- 조기 게임 오버 패널티: 점수 0만 체크하여 공격으로 일찍 끝난 경우를 구분하지 못함
- 장기 생존 보너스: 공격이 강해 일찍 끝나면 시간이 짧아 불리함
- 생존 능력과 공격 능력을 동시에 평가하여 각각의 목표가 혼동됨

### 하이브리드 접근 방법

#### 핵심 아이디어
생존 가중치와 공격 가중치를 분리하여 독립적으로 학습:
- **Step 1**: 솔로 플레이로 생존 가중치 학습 (공격 가중치 고정)
- **Step 2**: 대전 플레이로 공격 가중치 학습 (생존 가중치 고정)
- **Step 3**: 통합 검증 (대전 모드 최종 평가)

#### Step 1: 생존 가중치 학습 (솔로 플레이)

**평가 방식**: 보드 상태 기반 생존 능력 평가

**평가 지표**:
- 최대 보드 높이: 높이가 낮을수록 좋음 (패널티)
- 평균 보드 높이: 평균적으로 낮게 유지하면 보너스
- 구멍 수: 구멍이 적을수록 좋음 (패널티)
- 생존 시간: 오래 생존할수록 보너스
- 줄 삭제 수: 줄 삭제 보너스

**적합도 계산**:
```java
survivalFitness = (20.0 - avgMaxHeight) * 10.0      // 최대 높이 보너스
                + (20.0 - avgHeight) * 5.0          // 평균 높이 보너스
                + avgHoles * -5.0                   // 구멍 패널티
                + linesCleared * 100.0              // 줄 삭제 보너스
                + gameTimeMs / 1000.0 * 10.0        // 생존 시간 보너스
                + (gameTimeMs > 18750 ? 1000.0 : 0) // 장기 생존 보너스
```

**학습 대상**: 생존 가중치만 (Landing Height, EPCM, Row Transitions, Column Transitions, Holes, Well Sums)
**공격 가중치**: 기본값으로 고정

#### Step 2: 공격 가중치 학습 (대전 플레이)

**평가 방식**: AI vs AI 대전 모드

**평가 지표**:
- 승리율: 승리 시 큰 보너스
- 공격 효율: 2/3/4줄 삭제 보너스
- 줄 삭제 수: 평균 줄 삭제 수
- 점수: 평균 점수

**적합도 계산**: 기존 대전 모드 적합도 계산 방식 사용

**학습 대상**: 공격 가중치만 (Attack 2 Lines, Attack 3 Lines, Attack 4 Lines)
**생존 가중치**: Step 1 결과로 고정

#### Step 3: 통합 검증

**평가 방식**: 대전 모드 최종 평가
- Step 1과 Step 2에서 학습된 가중치를 결합
- 실제 대전 모드에서 성능 검증

### 구현 세부사항

#### WeightSet 확장
- `copySurvivalWeights()`: 생존 가중치만 복사
- `copyAttackWeights()`: 공격 가중치만 복사
- `updateSurvivalWeights()`: 생존 가중치만 업데이트
- `updateAttackWeights()`: 공격 가중치만 업데이트
- `randomSurvivalWeights()`: 생존 가중치만 랜덤 생성
- `randomAttackWeights()`: 공격 가중치만 랜덤 생성

#### FitnessEvaluator 확장
- `evaluateSurvival()`: 솔로 플레이 평가 (보드 상태 기반)
- `calculateAverageHeight()`: 평균 보드 높이 계산
- `countHoles()`: 구멍 개수 계산

#### HybridGeneticAlgorithm
- `learnSurvivalWeights()`: Step 1 실행
- `learnAttackWeights()`: Step 2 실행
- 두 단계의 세대별 결과를 별도로 관리

### 장점

1. **생존 능력 평가 명확**: 솔로 플레이에서 생존만 집중 평가
2. **공격 능력 평가 명확**: 대전 플레이에서 공격만 집중 평가
3. **학습 효율**: 각 목표를 독립적으로 최적화
4. **가중치 해석 용이**: 생존/공격 가중치를 분리하여 이해하기 쉬움
5. **유연한 조합**: 생존/공격 가중치를 독립적으로 조정 가능

### 단점

1. **구현 복잡**: 두 가지 평가 환경과 적합도 함수 필요
2. **가중치 간 상호작용 미반영**: 실제 대전에서는 생존과 공격이 동시에 작용
3. **학습 시간 증가**: 두 가지 평가를 모두 수행해야 함

### 기존 방식과의 비교

| 항목 | 통합 적합도 방식 | 하이브리드 방식 |
|------|----------------|----------------|
| **생존 능력 평가** | 어려움 (공격과 혼동) | 쉬움 (솔로 플레이) |
| **공격 능력 평가** | 쉬움 (대전 플레이) | 쉬움 (대전 플레이) |
| **가중치 상호작용** | 학습됨 | 학습 안 됨 |
| **실제 환경 반영** | 높음 (대전 모드) | 낮음 (분리된 환경) |
| **학습 속도** | 보통 | 느림 (두 배 평가) |
| **구현 복잡도** | 낮음 | 높음 |

### 하이브리드 학습 결과 (2025-12-01)

#### 훈련 설정
- **총 소요 시간**: 35.6분
- **Step 1 (생존 가중치)**: 5세대, 집단 크기 20
- **Step 2 (공격 가중치)**: 5세대, 집단 크기 20

#### Step 1 결과: 생존 가중치
- **최적 생존 가중치**:
  - Landing Height: -3.81
  - EPCM: 1.54
  - Row Transitions: -3.30
  - Column Transitions: -21.29
  - Holes: -15.78
  - Well Sums: -3.62

**특징**:
- Column Transitions가 매우 큰 패널티 (-21.29): 세로 전환을 최소화하여 보드를 단단하게 유지
- Holes 패널티가 큼 (-15.78): 구멍을 적극적으로 회피
- Well Sums 패널티가 작음 (-3.62): Well을 허용하여 I-block 전략 가능

#### Step 2 결과: 공격 가중치
- **최적 공격 가중치**:
  - Attack 2 Lines: 5.59
  - Attack 3 Lines: 36.11
  - Attack 4 Lines (Tetris): 42.12

**특징**:
- Tetris (4줄) 보너스가 가장 큼 (42.12)
- 3줄 삭제 보너스도 큼 (36.11)
- 2줄 삭제 보너스는 작음 (5.59): 2줄 삭제보다 3-4줄 삭제를 선호

#### 최종 통합 결과
- **적합도**: 3350.40
- **평균 줄 수**: 12.0
- **평균 점수**: 6005

**최종 가중치**:
```java
new WeightSet(-3.81, 1.54, -3.30, -21.29, -15.78, -3.62, 5.59, 36.11, 42.12)
```

#### 기존 방식과의 비교

| 항목 | 통합 적합도 방식 (이전) | 하이브리드 방식 (최신) |
|------|----------------------|---------------------|
| **평균 줄 수** | 4.0 | 12.0 |
| **평균 점수** | 2034 | 6005 |
| **적합도** | -2424.21 | 3350.40 |
| **Landing Height** | -7.80 | -3.81 |
| **EPCM** | 5.58 | 1.54 |
| **Column Transitions** | -1.13 | -21.29 |
| **Holes** | -18.80 | -15.78 |
| **Attack 4 Lines** | 59.16 | 42.12 |

**주요 차이점**:
1. **평균 줄 수 대폭 증가**: 4.0 → 12.0 (3배)
2. **평균 점수 증가**: 2034 → 6005 (약 3배)
3. **Column Transitions 패널티 증가**: -1.13 → -21.29 (약 19배)
4. **EPCM 감소**: 5.58 → 1.54 (약 0.28배)
5. **Attack 4 Lines 감소**: 59.16 → 42.12 (약 0.71배)

**분석**:
- 하이브리드 방식이 더 나은 성능을 보임 (평균 줄 수, 점수 모두 증가)
- Column Transitions 패널티가 크게 증가하여 보드를 더 단단하게 유지
- 공격 보너스는 다소 감소했지만, 생존 능력 향상으로 전체 성능이 개선됨

---

**최종 업데이트**: 2025-12-01
**작성자**: AI Training Documentation System

