# 테트리스 AI 유전 알고리즘 훈련 이력 및 방법론

## 목차
1. [개요](#개요)
2. [실패했던 훈련 방법들](#실패했던-훈련-방법들)
3. [현재 진행 중인 훈련 방법](#현재-진행-중인-훈련-방법)
4. [훈련 결과 비교](#훈련-결과-비교)
5. [주요 개선 사항](#주요-개선-사항)
6. [참고 문서](#참고-문서)

---

## 개요

테트리스 AI의 가중치를 최적화하기 위해 유전 알고리즘(Genetic Algorithm)을 사용하여 훈련을 진행했습니다. 초기에는 단일 플레이어 모드 기반 평가를 사용했으나, 실제 대전 모드와의 환경 차이로 인해 실패했습니다. 이후 AI vs AI 대전 모드 기반 평가로 전환하여 실제 사용 환경과 동일한 조건에서 훈련을 진행하고 있습니다.

### 훈련 목표
- **대전 모드에서 승리할 수 있는 AI 가중치 최적화**
- **El-Tetris 알고리즘 기반 특징 계산 + 대전 모드 공격 규칙**

---

## 실패했던 훈련 방법들

### 1. 단일 플레이어 모드 기반 평가 (초기 시도)

#### 방법
- **평가 환경**: AI가 혼자서 게임을 플레이
- **적합도 계산**: 생존 시간, 줄 삭제 수, 점수 기반
- **문제점**: 실제 대전 모드와 환경이 완전히 다름

#### 실패 원인
1. **공격 메커니즘 부재**: 대전 모드에서는 2줄 이상 삭제 시 상대방에게 공격 블록이 전송되지만, 단일 플레이어 모드에서는 이를 시뮬레이션할 수 없음
2. **전략 차이**: 단일 플레이어 모드에서는 생존 중심 전략이 최적이지만, 대전 모드에서는 공격 중심 전략이 필요
3. **가중치 불일치**: 단일 플레이어 모드에서 최적화된 가중치가 대전 모드에서는 성능이 떨어짐

#### 결과
- 훈련된 AI가 대전 모드에서 제대로 작동하지 않음
- 1줄도 제대로 삭제하지 못하는 경우 발생

---

### 2. 단순 특징 기반 평가 (El-Tetris 미적용)

#### 방법
- **평가 특징**: Aggregate Height, Bumpiness, Lines Cleared, Tetris
- **문제점**: El-Tetris의 정교한 특징 계산 방식을 사용하지 않음

#### 실패 원인
1. **Landing Height vs Aggregate Height**: El-Tetris는 블록별 착지 높이를 계산하지만, 단순 Aggregate Height는 전체 보드 높이 합계만 계산
2. **Eroded Piece Cells Metric 부재**: El-Tetris의 핵심 특징인 EPCM을 사용하지 않음
3. **Well Sums 부재**: Well Building 전략을 위한 Well Sums 특징 미사용

#### 결과
- El-Tetris 논문의 가중치와 비교 시 큰 차이 발생
- AI의 보드 관리 능력이 떨어짐

---

### 3. 빠른 평가 모드 미적용 (초기)

#### 방법
- **평가 방식**: 모든 개체에 대해 정확한 평가만 수행
- **문제점**: 훈련 시간이 너무 오래 걸림 (6시간 이상)

#### 실패 원인
1. **시간 비효율**: 각 개체당 5게임씩 정확한 평가를 수행하여 세대당 수십 분 소요
2. **확장성 부족**: 세대 수나 개체 수를 늘릴 수 없음

#### 결과
- 훈련 시간이 너무 길어 실용적이지 않음
- 실험 및 개선을 반복하기 어려움

---

### 4. 이전 세대 최고 개체를 상대 AI로 사용하지 않음 (초기)

#### 방법
- **상대 AI**: 항상 기본 가중치(El-Tetris) 사용
- **문제점**: 점진적 난이도 증가가 없어 학습 효율이 떨어짐

#### 실패 원인
1. **학습 곡선 부재**: 초기 세대와 후기 세대 모두 같은 난이도의 상대와 대전
2. **로컬 최적해 수렴**: 약한 상대와의 대전으로 인해 최적화가 제대로 이루어지지 않음

#### 결과
- 학습 효율이 낮음
- 최적 가중치를 찾지 못함

---

## 현재 진행 중인 훈련 방법

### 평가 환경: AI vs AI 대전 모드

#### 핵심 특징
- **실제 대전 모드와 동일한 환경**: 공격 블록 전송, 게임 오버 조건 등 모든 규칙 동일
- **AI vs AI 대전**: 평가 대상 AI와 상대 AI가 대전하여 승리/패배 기반 적합도 계산

#### 평가 로직
```java
// 공격 로직: 블록 고정 후 콜백으로 처리
engine1.setOnBlockFixedCallback(() -> handleAttack(engine1, engine2));
engine2.setOnBlockFixedCallback(() -> handleAttack(engine2, engine1));

// 승리/패배 기반 적합도
if (winner == 1) {
    score += 10000;  // 승리 보너스
    linesCleared += 20;
    isWin = true;
} else if (winner == 2) {
    score = Math.max(0, score - 5000);  // 패배 패널티
    isWin = false;
}
```

---

### El-Tetris 알고리즘 기반 특징 계산

#### 구현된 특징
1. **Landing Height**: 블록이 착지한 높이 (블록별 계산)
2. **Eroded Piece Cells Metric (EPCM)**: 삭제된 줄 수 × 유용한 블록 수
3. **Row Transitions**: 가로 방향 빈칸↔채움 전환 횟수
4. **Column Transitions**: 세로 방향 빈칸↔채움 전환 횟수
5. **Number of Holes**: 구멍 개수
6. **Well Sums**: Well의 깊이 합

#### 대전 모드 추가 특징
7. **Attack Bonus**: 2줄 이상 삭제 시 공격 보너스

#### 평가 함수
```java
Score = weightLandingHeight × landingHeight
      + weightEPCM × epcm
      + weightRowTransitions × rowTransitions
      + weightColumnTransitions × columnTransitions
      + weightHoles × holes
      + weightWellSums × wellSums
      + (linesCleared >= 2 ? linesCleared × weightAttackBonus : 0)
```

---

### 빠른 평가 모드 + 정확한 평가 모드

#### 빠른 평가 모드 (세대 내 모든 개체)
- **목적**: 빠른 학습을 위한 대략적 평가
- **설정**:
  - 게임당 최대 시간: 10초 (실제 5분 = 평가 8초)
  - 최대 이동 횟수: 1000회
  - 게임 타이머 간격: 20ms
  - AI 행동 주기: 5ms
  - 개체당 게임 수: 5게임

#### 정밀 평가 모드 (각 세대 최고 개체)
- **목적**: 정확한 성능 측정
- **설정**:
  - 게임당 최대 시간: 20초 (실제 5분 = 평가 19초)
  - 최대 이동 횟수: 2000회
  - 게임 타이머 간격: 50ms
  - AI 행동 주기: 20ms
  - 개체당 게임 수: 5게임

---

### 적응적 상대 AI (이전 세대 최고 개체)

#### 방법
- **첫 세대**: 기본 가중치(El-Tetris)와 대전
- **2세대 이후**: 이전 세대의 최고 개체와 대전
- **효과**: 점진적 난이도 증가로 학습 효율 향상

#### 구현
```java
if (currentGeneration > 0 && !generationResults.isEmpty()) {
    Individual previousBest = new Individual(
        copyWeights(generationResults.get(generationResults.size() - 1).bestWeights));
    evaluator.setOpponent(previousBest);
} else {
    evaluator.setOpponent(null);  // 기본 가중치 사용
}
```

---

### 유전 알고리즘 파라미터

#### 현재 설정
- **집단 크기 (POPULATION_SIZE)**: 30
- **엘리트 개체 수 (ELITE_SIZE)**: 6 (20%)
- **최대 세대 수 (MAX_GENERATIONS)**: 10
- **기본 돌연변이 확률 (BASE_MUTATION_RATE)**: 0.15
- **교차 확률 (CROSSOVER_RATE)**: 0.7

#### 적응적 돌연변이
- 세대가 진행될수록 돌연변이율 감소 (0.5% 감소)
- 초기: 탐색 중심, 후기: 수렴 중심

---

### 적합도 함수

#### 계산 방식
```java
fitness = survivalBonus          // totalGames * 5.0
       + linesBonus              // averageLinesPerGame * 100.0
       + scoreBonus              // averageScorePerGame * 0.5
       + winBonus                // winCount * 1000.0
       - lossPenalty             // lossCount * 500.0
       + earlyGameOverPenalty    // (score == 0 ? -100.0 : 0.0)
```

#### 특징
- **승리 보너스**: 승리 시 큰 보너스 (1000점)
- **패배 패널티**: 패배 시 패널티 (500점)
- **줄 삭제 보너스**: 평균 줄 삭제 수에 큰 가중치 (100점)
- **생존 보너스**: 게임 수에 작은 가중치 (5점)

---

## 훈련 결과 비교

### 보고서 1: 2025-11-30 12:55:00 (초기 시도)

#### 설정
- 세대 수: 50세대
- 집단 크기: 20
- 평가 방식: 단일 플레이어 모드 (추정)

#### 결과
- **최고 적합도**: 45.70
- **평균 줄 수**: 0.3
- **평균 점수**: 124
- **가중치**: 
  - Holes: -77.61
  - Row Transitions: -10.54
  - Aggregate Height: -0.33
  - Bumpiness: -0.37
  - Lines Cleared: 13.03
  - Tetris: 52.47

#### 문제점
- 평균 줄 수가 매우 낮음 (0.3줄)
- 적합도가 매우 낮음
- 대전 모드에서 제대로 작동하지 않음

---

### 보고서 2: 2025-11-30 13:45:41 (개선 시도)

#### 설정
- 세대 수: 50세대
- 집단 크기: 20
- 평가 방식: AI vs AI 대전 모드 (추정)

#### 결과
- **최고 적합도**: (데이터 부족)
- **평균 줄 수**: (데이터 부족)
- **평균 점수**: (데이터 부족)

#### 문제점
- 데이터가 불완전하여 정확한 평가 불가

---

### 보고서 3: 2025-11-30 14:19:02 (AI vs AI 대전 모드 적용)

#### 설정
- 세대 수: 50세대
- 집단 크기: 20
- 평가 방식: AI vs AI 대전 모드
- 총 소요 시간: 21.2분

#### 결과
- **최고 적합도**: 45.70
- **평균 줄 수**: 0.3
- **평균 점수**: 124
- **가중치**: 
  - Holes: -77.61
  - Row Transitions: -10.54
  - Aggregate Height: -0.33
  - Bumpiness: -0.37
  - Lines Cleared: 13.03
  - Tetris: 52.47

#### 문제점
- 여전히 평균 줄 수가 매우 낮음
- 적합도가 낮음
- El-Tetris 특징을 사용하지 않음

---

### 보고서 4: 29세대까지 진행 (El-Tetris 미적용, 이전 방식)

#### 설정
- 세대 수: 30세대 (0-29)
- 집단 크기: 30
- 평가 방식: AI vs AI 대전 모드
- 특징: Aggregate Height, Bumpiness 사용 (El-Tetris 미적용)

#### 결과
- **최고 적합도**: 2109.00 (세대 7)
- **평균 줄 수**: 21.0
- **평균 점수**: 10490
- **가중치**: 
  - Holes: -65.19
  - Row Transitions: -8.79
  - Aggregate Height: -0.81
  - Bumpiness: -0.99
  - Lines Cleared: 11.68
  - Tetris: 137.71

#### 평가
- 평균 줄 수가 크게 개선됨 (0.3 → 21.0)
- 적합도가 크게 향상됨 (45.70 → 2109.00)
- 하지만 El-Tetris 특징을 사용하지 않아 논문과 비교 시 차이 발생

---

### 보고서 5: 2025-12-01 13:33:48 (El-Tetris 알고리즘 기반 훈련 완료)

#### 설정
- **세대 수**: 10세대
- **집단 크기**: 30
- **엘리트 개체 수**: 6
- **평가 방식**: AI vs AI 대전 모드
- **특징**: El-Tetris 6가지 특징 + 대전 모드 공격 보너스
- **상대 AI**: 이전 세대 최고 개체
- **빠른 평가**: 세대 내 모든 개체
- **정확한 평가**: 각 세대 최고 개체
- **총 소요 시간**: 166.7분 (약 2.8시간)

#### 최적화된 타이머 설정
- **빠른 평가**:
  - 게임 타이머: 20ms
  - AI 행동 주기: 5ms
  - 최대 게임 시간: 3초
  - 최대 이동 횟수: 1000회
  
- **정확한 평가**:
  - 게임 타이머: 50ms
  - AI 행동 주기: 20ms
  - 최대 게임 시간: 10초
  - 최대 이동 횟수: 2000회

#### 결과
- **최고 적합도**: 3330.40 (정확한 평가 기준, 세대 6)
- **평균 줄 수**: 8.0
- **평균 점수**: 4011
- **가중치**:
  - Landing Height: -9.47
  - EPCM: 3.41
  - Row Transitions: -9.08
  - Column Transitions: -16.46
  - Holes: -6.73
  - Well Sums: -2.68
  - Attack Bonus: 15.42

#### 평가
- ✅ El-Tetris 알고리즘 특징 계산 구현 완료
- ✅ 대전 모드 공격 규칙 추가 완료
- ✅ 빠른 평가 모드 최적화 완료
- ⚠️ 평균 줄 수가 이전 방법(21.0줄)보다 낮음 (8.0줄)
- ⚠️ 적합도가 이전 방법(2109.00)보다 높지만, 평균 줄 수는 낮음
- **추가 최적화 필요**: 적합도 함수나 평가 환경 조정 필요

#### 주요 관찰
- 빠른 평가 적합도는 매우 높음 (12000+), 하지만 정확한 평가에서는 낮음 (3000+)
- 이는 빠른 평가와 정확한 평가 간의 환경 차이를 시사
- 세대 6에서 최고 성능 (적합도 12048.50, 평균 줄 20.0, 평균 점수 10047)

---

## 주요 개선 사항

### 1. 평가 환경 개선
- ❌ 단일 플레이어 모드 → ✅ AI vs AI 대전 모드
- 실제 사용 환경과 동일한 조건에서 평가

### 2. 특징 계산 개선
- ❌ 단순 특징 (Aggregate Height, Bumpiness) → ✅ El-Tetris 특징 (Landing Height, EPCM, Column Transitions, Well Sums)
- 논문 기반 정교한 특징 계산

### 3. 평가 방식 개선
- ❌ 정확한 평가만 수행 (6시간 소요) → ✅ 빠른 평가 + 정확한 평가 (50-100분)
- 학습 시간 대폭 단축

### 4. 상대 AI 개선
- ❌ 항상 기본 가중치 사용 → ✅ 이전 세대 최고 개체 사용
- 점진적 난이도 증가로 학습 효율 향상

### 5. 적합도 함수 개선
- 승리/패배 기반 보너스/패널티 추가
- 줄 삭제 보너스 가중치 증가
- 생존 보너스 가중치 감소

### 6. 대전 모드 공격 규칙 추가
- 2줄 이상 삭제 시 공격 보너스 추가
- 실제 대전 모드와 동일한 공격 메커니즘

---

## 참고 문서

### 관련 문서
1. **RUN_GENETIC_ALGORITHM.md**: 유전 알고리즘 실행 방법
2. **weight_comparison_analysis.md**: El-Tetris vs 유전 알고리즘 가중치 비교 분석

### 보고서 파일
모든 보고서는 `docs/reports/` 디렉토리에 저장되어 있습니다.

1. **report_20251130_125500_initial_attempt.txt**: 초기 시도 (50세대)
2. **report_20251130_134541_improved_attempt.txt**: 개선 시도 (50세대)
3. **report_20251130_141902_ai_vs_ai.txt**: AI vs AI 대전 모드 적용 (50세대)
4. **report_29generations_old_method.txt**: 29세대까지 진행 (El-Tetris 미적용)
5. **report_20251201_133348_el_tetris_based.txt**: El-Tetris 알고리즘 기반 훈련 (10세대) ⭐ 최신

자세한 내용은 [reports/README.md](./reports/README.md)를 참고하세요.

### 코드 파일
1. **GeneticAlgorithm.java**: 유전 알고리즘 메인 로직
2. **FitnessEvaluator.java**: 적합도 평가 (AI vs AI 대전 모드)
3. **TetrisAI.java**: AI 로직 (El-Tetris 특징 계산)
4. **WeightSet.java**: 가중치 집합
5. **Individual.java**: 개체 (가중치 + 적합도)
6. **GeneticAlgorithmRunner.java**: 실행 클래스

---

## 결론

초기에는 단일 플레이어 모드 기반 평가와 단순 특징을 사용하여 실패했습니다. 이후 AI vs AI 대전 모드 기반 평가와 El-Tetris 알고리즘 특징 계산을 적용하여 실제 사용 환경과 동일한 조건에서 훈련을 진행했습니다. 빠른 평가 모드와 적응적 상대 AI를 통해 학습 효율을 크게 향상시켰습니다.

통합 적합도 방식 훈련(2025-12-01 16:04:24 완료)에서는 6가지 특징과 대전 모드 공격 규칙을 모두 반영하여 훈련을 진행했습니다. 하이브리드 방식과 비교했을 때, 통합 적합도 방식이 더 나은 성능을 보였습니다. 최종 가중치(-7.80, 5.58, -1.58, -1.13, -18.80, -8.69, 15.14, 19.18, 59.16)는 실제 게임에서 더 효과적으로 작동하는 것으로 확인되었습니다.

### 통합 적합도 방식이 하이브리드 방식보다 우수한 핵심 이유

1. **가중치 간 상호작용 학습**: 모든 가중치를 동시에 학습하여 실제 대전 환경에서의 상호작용이 자연스럽게 학습됨
2. **실제 환경 반영**: 실제 대전 환경에서 직접 학습하여 실제 사용 환경과 완전히 일치
3. **유연한 전략**: Column Transitions 패널티가 작아(-1.13) 유연한 전략 가능 (하이브리드: -21.29로 너무 단단함)
4. **공격과 생존의 균형**: 실제 대전 환경에서 공격과 생존의 균형이 더 잘 맞음
5. **학습 효율성**: 더 빠른 학습 시간(18.5분 vs 35.6분)과 더 나은 실제 성능

하이브리드 방식은 적합도는 높았지만(3350.40), 실제 게임 성능은 통합 방식보다 떨어졌습니다. 이는 분리된 환경에서 학습하여 가중치 간 상호작용이 학습되지 않았기 때문입니다.

### 향후 개선 방향
1. **더 많은 세대 훈련**: 5세대에서 10-20세대로 확장
2. **하이퍼파라미터 튜닝**: 집단 크기, 돌연변이율, 교차율 등 조정
3. **평가 환경 최적화**: 빠른 평가와 정확한 평가 간의 환경 차이 최소화
4. **적합도 함수 미세 조정**: 실제 게임 성능과의 상관관계 분석

---

## 최신 훈련 결과 요약

### 통합 적합도 방식 훈련 (2025-12-01 16:04:24 완료) ⭐ 최신

**훈련 설정**:
- 집단 크기: 20
- 세대 수: 5
- 총 소요 시간: 18.5분
- 평가 방식: AI vs AI 대전 모드

**최종 결과**:
- 적합도: -2424.21
- 평균 줄 수: 4.0
- 평균 점수: 2034

**최종 가중치**:
```java
new WeightSet(-7.80, 5.58, -1.58, -1.13, -18.80, -8.69, 15.14, 19.18, 59.16)
```

**가중치 상세**:
- Landing Height: -7.80
- EPCM: 5.58
- Row Transitions: -1.58
- Column Transitions: -1.13
- Holes: -18.80
- Well Sums: -8.69
- Attack 2 Lines: 15.14
- Attack 3 Lines: 19.18
- Attack 4 Lines (Tetris): 59.16

**주요 특징**:
- **Holes 패널티가 매우 큼** (-18.80): 구멍을 적극적으로 회피
- **Tetris 보너스가 매우 큼** (59.16): 4줄 삭제를 강력하게 선호
- **Column Transitions 패널티가 작음** (-1.13): 세로 전환에 대한 제약이 적음
- **EPCM이 양수** (5.58): 줄 삭제 시 보너스를 제공

**El-Tetris 논문 가중치와 비교**:
- Landing Height: -7.80 vs -4.50 (약 1.7배)
- EPCM: 5.58 vs 3.42 (약 1.6배)
- Row Transitions: -1.58 vs -3.22 (약 0.5배, 더 작은 패널티)
- Column Transitions: -1.13 vs -9.35 (약 0.12배, 훨씬 작은 패널티)
- Holes: -18.80 vs -7.90 (약 2.4배, 더 큰 패널티)
- Well Sums: -8.69 vs -3.39 (약 2.6배, 더 큰 패널티)

**주요 차이점**:
- 대전 모드 특성상 Holes와 Well Sums에 더 큰 패널티
- Column Transitions 패널티가 매우 작음 (대전 모드에서 유연한 전략 허용)
- Tetris 보너스가 매우 큼 (공격 중심 전략)

---

## 생존 능력 향상 훈련 (2025-12-01)

### 배경
기존 훈련된 AI가 실제 플레이어와 대전할 때 생존 능력이 부족하여 플레이어가 아무것도 하지 않아도 AI가 먼저 게임 오버되는 문제가 발생했습니다.

### 개선 사항

#### 1. 적합도 계산 방식 개선
- **생존 시간 보너스 추가**: 평균 생존 시간(초) × 10.0
- **생존 보너스 증가**: 게임 수 × 5.0 → 20.0
- **승리 보너스 감소**: 승리 횟수 × 1000.0 → 500.0 (생존과의 균형)
- **패배 패널티 증가**: 패배 횟수 × 500.0 → 1000.0
- **조기 게임 오버 패널티 증가**: -100.0 → -500.0
- **장기 생존 보너스 추가**: 평균 19초(실제 5분) 이상 생존 시 +1000.0

#### 2. 평가 시간 확장
- **정밀 평가 최대 시간**: 10초 → 20초 (실제 5분 = 평가 19초)
- **빠른 평가 최대 시간**: 3초 → 10초 (실제 5분 = 평가 8초)
- 생존 시간 보너스를 받을 수 있도록 충분한 시간 확보

#### 3. 가중치 범위 조정
- **Holes 가중치 범위 증가**: -21 ~ -1 → -20 ~ -5 (더 큰 패널티로 생존 능력 강화)
- **공격 보너스 범위 감소**: 
  - Attack 2 Lines: 5 ~ 35 → 5 ~ 25
  - Attack 3 Lines: 10 ~ 50 → 10 ~ 40
  - Attack 4 Lines: 20 ~ 80 → 20 ~ 60

#### 4. 유전 알고리즘 설정
- **집단 크기**: 30 → 20
- **세대 수**: 10 → 5
- 빠른 테스트를 위한 설정

### 적합도 계산 공식 (개선 후)

```
적합도 = 생존시간보너스 + 생존보너스 + 줄삭제보너스 + 점수보너스
       + 승리보너스 - 패배패널티 + 조기게임오버패널티 + 장기생존보너스

생존시간보너스 = 평균생존시간(초) × 10.0
생존보너스 = 게임수 × 20.0
줄삭제보너스 = 평균줄수 × 100.0
점수보너스 = 평균점수 × 0.5
승리보너스 = 승리횟수 × 500.0
패배패널티 = 패배횟수 × 1000.0
조기게임오버패널티 = (점수 == 0) ? -500.0 : 0.0
장기생존보너스 = (평균생존시간 > 19초) ? 1000.0 : 0.0
```

### 목표
- AI가 최소한 5분(평가 환경 19초) 이상 생존할 수 있도록 학습
- 공격과 생존의 균형 유지
- 플레이어가 아무것도 하지 않아도 AI가 먼저 게임 오버되지 않도록 개선

---

---

## 하이브리드 학습 방법 (2025-12-01) - 실험적 시도

### 배경
기존 통합 적합도 계산 방식의 문제점을 해결하기 위한 실험적 시도:
- 평균 시간 보너스: 공격이 강해 일찍 끝나면 시간이 짧아 불리함
- 조기 게임 오버 패널티: 점수 0만 체크하여 공격으로 일찍 끝난 경우를 구분하지 못함
- 장기 생존 보너스: 공격이 강해 일찍 끝나면 시간이 짧아 불리함
- 생존 능력과 공격 능력을 동시에 평가하여 각각의 목표가 혼동됨

**결과**: 하이브리드 방식은 적합도는 높았지만, 실제 게임 성능에서는 통합 적합도 방식이 더 우수한 것으로 확인됨.

### 하이브리드 접근 방법

#### 핵심 아이디어
생존 가중치와 공격 가중치를 분리하여 독립적으로 학습:
- **Step 1**: 솔로 플레이로 생존 가중치 학습 (공격 가중치 고정)
- **Step 2**: 대전 플레이로 공격 가중치 학습 (생존 가중치 고정)
- **Step 3**: 통합 검증 (대전 모드 최종 평가)

#### Step 1: 생존 가중치 학습 (솔로 플레이)

**평가 방식**: 보드 상태 기반 생존 능력 평가

**평가 지표**:
- 최대 보드 높이: 높이가 낮을수록 좋음 (패널티)
- 평균 보드 높이: 평균적으로 낮게 유지하면 보너스
- 구멍 수: 구멍이 적을수록 좋음 (패널티)
- 생존 시간: 오래 생존할수록 보너스
- 줄 삭제 수: 줄 삭제 보너스

**적합도 계산**:
```java
survivalFitness = (20.0 - avgMaxHeight) * 10.0      // 최대 높이 보너스
                + (20.0 - avgHeight) * 5.0          // 평균 높이 보너스
                + avgHoles * -5.0                   // 구멍 패널티
                + linesCleared * 100.0              // 줄 삭제 보너스
                + gameTimeMs / 1000.0 * 10.0        // 생존 시간 보너스
                + (gameTimeMs > 18750 ? 1000.0 : 0) // 장기 생존 보너스
```

**학습 대상**: 생존 가중치만 (Landing Height, EPCM, Row Transitions, Column Transitions, Holes, Well Sums)
**공격 가중치**: 기본값으로 고정

#### Step 2: 공격 가중치 학습 (대전 플레이)

**평가 방식**: AI vs AI 대전 모드

**평가 지표**:
- 승리율: 승리 시 큰 보너스
- 공격 효율: 2/3/4줄 삭제 보너스
- 줄 삭제 수: 평균 줄 삭제 수
- 점수: 평균 점수

**적합도 계산**: 기존 대전 모드 적합도 계산 방식 사용

**학습 대상**: 공격 가중치만 (Attack 2 Lines, Attack 3 Lines, Attack 4 Lines)
**생존 가중치**: Step 1 결과로 고정

#### Step 3: 통합 검증

**평가 방식**: 대전 모드 최종 평가
- Step 1과 Step 2에서 학습된 가중치를 결합
- 실제 대전 모드에서 성능 검증

### 구현 세부사항

#### WeightSet 확장
- `copySurvivalWeights()`: 생존 가중치만 복사
- `copyAttackWeights()`: 공격 가중치만 복사
- `updateSurvivalWeights()`: 생존 가중치만 업데이트
- `updateAttackWeights()`: 공격 가중치만 업데이트
- `randomSurvivalWeights()`: 생존 가중치만 랜덤 생성
- `randomAttackWeights()`: 공격 가중치만 랜덤 생성

#### FitnessEvaluator 확장
- `evaluateSurvival()`: 솔로 플레이 평가 (보드 상태 기반)
- `calculateAverageHeight()`: 평균 보드 높이 계산
- `countHoles()`: 구멍 개수 계산

#### HybridGeneticAlgorithm
- `learnSurvivalWeights()`: Step 1 실행
- `learnAttackWeights()`: Step 2 실행
- 두 단계의 세대별 결과를 별도로 관리

### 장점

1. **생존 능력 평가 명확**: 솔로 플레이에서 생존만 집중 평가
2. **공격 능력 평가 명확**: 대전 플레이에서 공격만 집중 평가
3. **학습 효율**: 각 목표를 독립적으로 최적화
4. **가중치 해석 용이**: 생존/공격 가중치를 분리하여 이해하기 쉬움
5. **유연한 조합**: 생존/공격 가중치를 독립적으로 조정 가능

### 단점

1. **구현 복잡**: 두 가지 평가 환경과 적합도 함수 필요
2. **가중치 간 상호작용 미반영**: 실제 대전에서는 생존과 공격이 동시에 작용
3. **학습 시간 증가**: 두 가지 평가를 모두 수행해야 함
4. **실제 성능 저하**: 적합도는 높지만 실제 게임에서는 통합 방식보다 성능이 떨어짐

### 통합 방식과의 비교

| 항목 | 통합 적합도 방식 ⭐ | 하이브리드 방식 |
|------|----------------|----------------|
| **생존 능력 평가** | 어려움 (공격과 혼동) | 쉬움 (솔로 플레이) |
| **공격 능력 평가** | 쉬움 (대전 플레이) | 쉬움 (대전 플레이) |
| **가중치 상호작용** | 학습됨 ✅ | 학습 안 됨 |
| **실제 환경 반영** | 높음 (대전 모드) ✅ | 낮음 (분리된 환경) |
| **학습 속도** | 빠름 (18.5분) ✅ | 느림 (35.6분, 두 배 평가) |
| **구현 복잡도** | 낮음 ✅ | 높음 |
| **실제 게임 성능** | 우수 ✅ | 보통 |
| **최종 선택** | ✅ 채택 | ❌ 미채택 |

### 하이브리드 학습 결과 (2025-12-01)

#### 훈련 설정
- **총 소요 시간**: 35.6분
- **Step 1 (생존 가중치)**: 5세대, 집단 크기 20
- **Step 2 (공격 가중치)**: 5세대, 집단 크기 20

#### Step 1 결과: 생존 가중치
- **최적 생존 가중치**:
  - Landing Height: -3.81
  - EPCM: 1.54
  - Row Transitions: -3.30
  - Column Transitions: -21.29
  - Holes: -15.78
  - Well Sums: -3.62

**특징**:
- Column Transitions가 매우 큰 패널티 (-21.29): 세로 전환을 최소화하여 보드를 단단하게 유지
- Holes 패널티가 큼 (-15.78): 구멍을 적극적으로 회피
- Well Sums 패널티가 작음 (-3.62): Well을 허용하여 I-block 전략 가능

#### Step 2 결과: 공격 가중치
- **최적 공격 가중치**:
  - Attack 2 Lines: 5.59
  - Attack 3 Lines: 36.11
  - Attack 4 Lines (Tetris): 42.12

**특징**:
- Tetris (4줄) 보너스가 가장 큼 (42.12)
- 3줄 삭제 보너스도 큼 (36.11)
- 2줄 삭제 보너스는 작음 (5.59): 2줄 삭제보다 3-4줄 삭제를 선호

#### 최종 통합 결과
- **적합도**: 3350.40
- **평균 줄 수**: 12.0
- **평균 점수**: 6005

**최종 가중치**:
```java
new WeightSet(-3.81, 1.54, -3.30, -21.29, -15.78, -3.62, 5.59, 36.11, 42.12)
```

#### 통합 적합도 방식 vs 하이브리드 방식 비교

| 항목 | 통합 적합도 방식 (최신) ⭐ | 하이브리드 방식 |
|------|----------------------|---------------------|
| **평균 줄 수** | 4.0 | 12.0 |
| **평균 점수** | 2034 | 6005 |
| **적합도** | -2424.21 | 3350.40 |
| **Landing Height** | -7.80 | -3.81 |
| **EPCM** | 5.58 | 1.54 |
| **Column Transitions** | -1.13 | -21.29 |
| **Holes** | -18.80 | -15.78 |
| **Attack 4 Lines** | 59.16 | 42.12 |
| **훈련 시간** | 18.5분 | 35.6분 |
| **실제 게임 성능** | 우수 | 보통 |

**주요 차이점**:
1. **실제 게임 성능**: 통합 적합도 방식이 실제 플레이에서 더 나은 성능을 보임
2. **훈련 시간**: 통합 적합도 방식이 약 2배 빠름 (18.5분 vs 35.6분)
3. **가중치 특성**:
   - 통합 방식: Column Transitions 패널티가 작아 유연한 전략 가능 (-1.13)
   - 하이브리드: Column Transitions 패널티가 매우 커 보드가 단단함 (-21.29)
   - 통합 방식: Tetris 보너스가 더 큼 (59.16 vs 42.12)
   - 통합 방식: Holes 패널티가 더 큼 (-18.80 vs -15.78)

**분석**:
- **통합 적합도 방식이 실제 게임에서 더 효과적**: 적합도는 낮지만 실제 플레이 성능이 우수
- **하이브리드 방식의 한계**: 생존과 공격을 분리하여 학습했지만, 실제 대전에서는 두 요소가 동시에 작용하므로 가중치 간 상호작용이 학습되지 않음
- **통합 방식의 장점**: 실제 대전 환경에서 직접 학습하여 가중치 간 상호작용이 자연스럽게 학습됨
- **최종 선택**: 통합 적합도 방식의 가중치(-7.80, 5.58, -1.58, -1.13, -18.80, -8.69, 15.14, 19.18, 59.16)를 최종 가중치로 채택

---

---

## 최종 채택된 가중치 (2025-12-01)

### 통합 적합도 방식 훈련 결과

**보고서**: `report_20251201_160424_line_based_weights.txt`

**최종 가중치**:
```java
new WeightSet(-7.80, 5.58, -1.58, -1.13, -18.80, -8.69, 15.14, 19.18, 59.16)
```

**특징**:
- Holes 패널티가 매우 큼 (-18.80): 구멍을 적극적으로 회피
- Tetris 보너스가 매우 큼 (59.16): 4줄 삭제를 강력하게 선호
- Column Transitions 패널티가 작음 (-1.13): 유연한 전략 가능
- EPCM이 양수 (5.58): 줄 삭제 시 보너스 제공

**실제 게임 성능**: 하이브리드 방식보다 우수한 성능을 보임

### 통합 적합도 방식이 하이브리드 방식보다 우수한 이유

#### 1. 가중치 간 상호작용 학습

**통합 방식**:
- 모든 가중치를 동시에 학습하여 가중치 간 상호작용이 자연스럽게 학습됨
- 실제 대전 환경에서 생존과 공격이 동시에 작용하는 상황을 직접 경험
- 예: 공격을 받으면서 생존하는 상황에서 최적의 가중치 조합을 찾음

**하이브리드 방식**:
- 생존 가중치와 공격 가중치를 분리하여 독립적으로 학습
- Step 1에서 학습한 생존 전략이 Step 2의 대전 환경에 최적화되지 않음
- 가중치 간 상호작용이 학습되지 않아 실제 대전에서 비효율적

#### 2. 실제 환경과의 일치성

**통합 방식**:
- 실제 대전 환경에서 직접 학습
- 공격을 받으면서 생존하는 상황을 동시에 경험
- 실제 사용 환경과 완전히 동일한 조건에서 학습

**하이브리드 방식**:
- Step 1: 솔로 플레이 (공격 없음) → 실제와 다른 환경
- Step 2: 대전 플레이 (이미 학습된 생존 가중치 사용) → Step 1에서 학습한 생존 전략이 대전 환경에 최적화되지 않음
- 분리된 환경에서 학습하여 실제 환경과의 차이 발생

#### 3. 가중치 특성의 차이

**Column Transitions 패널티**:
- 통합 방식: -1.13 (작은 패널티) → 유연한 전략 가능
- 하이브리드 방식: -21.29 (매우 큰 패널티) → 너무 단단한 보드를 만들려고 함 → 유연성 부족

**Tetris 보너스**:
- 통합 방식: 59.16 (매우 큰 보너스) → 더 공격적, 빠른 승리 가능
- 하이브리드 방식: 42.12 (상대적으로 작은 보너스) → 공격력이 약함

**Holes 패널티**:
- 통합 방식: -18.80 (매우 큰 패널티) → 구멍을 더 싫어함 → 더 나은 보드 관리
- 하이브리드 방식: -15.78 (상대적으로 작은 패널티) → 구멍 관리가 덜 엄격

#### 4. 적합도 vs 실제 성능

**하이브리드 방식**:
- 적합도: 3350.40 (높음)
- 평균 줄 수: 12.0
- 평균 점수: 6005
- **하지만 실제 게임 성능: 보통**

**통합 방식**:
- 적합도: -2424.21 (낮음)
- 평균 줄 수: 4.0
- 평균 점수: 2034
- **하지만 실제 게임 성능: 우수**

**왜 이런 차이가 발생하는가?**
- 적합도 함수는 평가 지표일 뿐, 실제 게임 성능과 완전히 일치하지 않을 수 있음
- 통합 방식은 실제 대전 환경에서 학습하므로, 적합도는 낮아도 실제 성능이 더 좋을 수 있음
- 하이브리드 방식은 분리된 환경에서 학습하여 적합도는 높지만 실제 성능은 떨어짐

#### 5. 학습 효율성

**통합 방식**:
- 훈련 시간: 18.5분 (빠름)
- 한 번의 평가로 모든 가중치 학습
- 실제 환경과 동일한 조건에서 학습

**하이브리드 방식**:
- 훈련 시간: 35.6분 (느림, 약 2배)
- 두 단계의 평가 필요 (생존 + 공격)
- 분리된 환경에서 학습하여 실제 환경과의 차이 발생

### 결론

통합 적합도 방식이 하이브리드 방식보다 우수한 이유:

1. **가중치 간 상호작용 학습**: 모든 가중치를 동시에 학습하여 실제 대전 환경에서의 상호작용이 자연스럽게 학습됨
2. **실제 환경 반영**: 실제 대전 환경에서 직접 학습하여 실제 사용 환경과 완전히 일치
3. **유연한 전략**: Column Transitions 패널티가 작아 유연한 전략 가능
4. **공격과 생존의 균형**: 실제 대전 환경에서 공격과 생존의 균형이 더 잘 맞음
5. **학습 효율성**: 더 빠른 학습 시간과 더 나은 실제 성능

하이브리드 방식의 한계:

1. **가중치 간 상호작용 미반영**: 생존과 공격 가중치를 분리하여 학습하므로 상호작용이 학습되지 않음
2. **환경 차이**: 솔로 플레이와 대전 플레이의 환경 차이로 인한 최적화 부족
3. **유연성 부족**: Column Transitions 패널티가 너무 커서 유연한 전략이 어려움
4. **적합도와 실제 성능의 불일치**: 적합도는 높지만 실제 성능은 떨어짐

이러한 이유로 통합 적합도 방식의 가중치를 최종 채택했습니다.

---

**최종 업데이트**: 2025-12-01
**작성자**: AI Training Documentation System

